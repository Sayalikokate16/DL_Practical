{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24050f30",
   "metadata": {
    "id": "24050f30"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import BatchNormalization, MaxPooling2D, Conv2D, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3476efd",
   "metadata": {
    "id": "c3476efd",
    "outputId": "11925600-2d37-4728-9d0f-05c43080de09"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.112522</td>\n",
       "      <td>-2.827204</td>\n",
       "      <td>-3.773897</td>\n",
       "      <td>-4.349751</td>\n",
       "      <td>-4.376041</td>\n",
       "      <td>-3.474986</td>\n",
       "      <td>-2.181408</td>\n",
       "      <td>-1.818286</td>\n",
       "      <td>-1.250522</td>\n",
       "      <td>-0.477492</td>\n",
       "      <td>...</td>\n",
       "      <td>0.792168</td>\n",
       "      <td>0.933541</td>\n",
       "      <td>0.796958</td>\n",
       "      <td>0.578621</td>\n",
       "      <td>0.257740</td>\n",
       "      <td>0.228077</td>\n",
       "      <td>0.123431</td>\n",
       "      <td>0.925286</td>\n",
       "      <td>0.193137</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.100878</td>\n",
       "      <td>-3.996840</td>\n",
       "      <td>-4.285843</td>\n",
       "      <td>-4.506579</td>\n",
       "      <td>-4.022377</td>\n",
       "      <td>-3.234368</td>\n",
       "      <td>-1.566126</td>\n",
       "      <td>-0.992258</td>\n",
       "      <td>-0.754680</td>\n",
       "      <td>0.042321</td>\n",
       "      <td>...</td>\n",
       "      <td>0.538356</td>\n",
       "      <td>0.656881</td>\n",
       "      <td>0.787490</td>\n",
       "      <td>0.724046</td>\n",
       "      <td>0.555784</td>\n",
       "      <td>0.476333</td>\n",
       "      <td>0.773820</td>\n",
       "      <td>1.119621</td>\n",
       "      <td>-1.436250</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.567088</td>\n",
       "      <td>-2.593450</td>\n",
       "      <td>-3.874230</td>\n",
       "      <td>-4.584095</td>\n",
       "      <td>-4.187449</td>\n",
       "      <td>-3.151462</td>\n",
       "      <td>-1.742940</td>\n",
       "      <td>-1.490659</td>\n",
       "      <td>-1.183580</td>\n",
       "      <td>-0.394229</td>\n",
       "      <td>...</td>\n",
       "      <td>0.886073</td>\n",
       "      <td>0.531452</td>\n",
       "      <td>0.311377</td>\n",
       "      <td>-0.021919</td>\n",
       "      <td>-0.713683</td>\n",
       "      <td>-0.532197</td>\n",
       "      <td>0.321097</td>\n",
       "      <td>0.904227</td>\n",
       "      <td>-0.421797</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.490473</td>\n",
       "      <td>-1.914407</td>\n",
       "      <td>-3.616364</td>\n",
       "      <td>-4.318823</td>\n",
       "      <td>-4.268016</td>\n",
       "      <td>-3.881110</td>\n",
       "      <td>-2.993280</td>\n",
       "      <td>-1.671131</td>\n",
       "      <td>-1.333884</td>\n",
       "      <td>-0.965629</td>\n",
       "      <td>...</td>\n",
       "      <td>0.350816</td>\n",
       "      <td>0.499111</td>\n",
       "      <td>0.600345</td>\n",
       "      <td>0.842069</td>\n",
       "      <td>0.952074</td>\n",
       "      <td>0.990133</td>\n",
       "      <td>1.086798</td>\n",
       "      <td>1.403011</td>\n",
       "      <td>-0.383564</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.800232</td>\n",
       "      <td>-0.874252</td>\n",
       "      <td>-2.384761</td>\n",
       "      <td>-3.973292</td>\n",
       "      <td>-4.338224</td>\n",
       "      <td>-3.802422</td>\n",
       "      <td>-2.534510</td>\n",
       "      <td>-1.783423</td>\n",
       "      <td>-1.594450</td>\n",
       "      <td>-0.753199</td>\n",
       "      <td>...</td>\n",
       "      <td>1.148884</td>\n",
       "      <td>0.958434</td>\n",
       "      <td>1.059025</td>\n",
       "      <td>1.371682</td>\n",
       "      <td>1.277392</td>\n",
       "      <td>0.960304</td>\n",
       "      <td>0.971020</td>\n",
       "      <td>1.614392</td>\n",
       "      <td>1.421456</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4993</th>\n",
       "      <td>0.608558</td>\n",
       "      <td>-0.335651</td>\n",
       "      <td>-0.990948</td>\n",
       "      <td>-1.784153</td>\n",
       "      <td>-2.626145</td>\n",
       "      <td>-2.957065</td>\n",
       "      <td>-2.931897</td>\n",
       "      <td>-2.664816</td>\n",
       "      <td>-2.090137</td>\n",
       "      <td>-1.461841</td>\n",
       "      <td>...</td>\n",
       "      <td>1.757705</td>\n",
       "      <td>2.291923</td>\n",
       "      <td>2.704595</td>\n",
       "      <td>2.451519</td>\n",
       "      <td>2.017396</td>\n",
       "      <td>1.704358</td>\n",
       "      <td>1.688542</td>\n",
       "      <td>1.629593</td>\n",
       "      <td>1.342651</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4994</th>\n",
       "      <td>-2.060402</td>\n",
       "      <td>-2.860116</td>\n",
       "      <td>-3.405074</td>\n",
       "      <td>-3.748719</td>\n",
       "      <td>-3.513561</td>\n",
       "      <td>-3.006545</td>\n",
       "      <td>-2.234850</td>\n",
       "      <td>-1.593270</td>\n",
       "      <td>-1.075279</td>\n",
       "      <td>-0.976047</td>\n",
       "      <td>...</td>\n",
       "      <td>1.388947</td>\n",
       "      <td>2.079675</td>\n",
       "      <td>2.433375</td>\n",
       "      <td>2.159484</td>\n",
       "      <td>1.819747</td>\n",
       "      <td>1.534767</td>\n",
       "      <td>1.696818</td>\n",
       "      <td>1.483832</td>\n",
       "      <td>1.047612</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>-1.122969</td>\n",
       "      <td>-2.252925</td>\n",
       "      <td>-2.867628</td>\n",
       "      <td>-3.358605</td>\n",
       "      <td>-3.167849</td>\n",
       "      <td>-2.638360</td>\n",
       "      <td>-1.664162</td>\n",
       "      <td>-0.935655</td>\n",
       "      <td>-0.866953</td>\n",
       "      <td>-0.645363</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.472419</td>\n",
       "      <td>-1.310147</td>\n",
       "      <td>-2.029521</td>\n",
       "      <td>-3.221294</td>\n",
       "      <td>-4.176790</td>\n",
       "      <td>-4.009720</td>\n",
       "      <td>-2.874136</td>\n",
       "      <td>-2.008369</td>\n",
       "      <td>-1.808334</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>-0.547705</td>\n",
       "      <td>-1.889545</td>\n",
       "      <td>-2.839779</td>\n",
       "      <td>-3.457912</td>\n",
       "      <td>-3.929149</td>\n",
       "      <td>-3.966026</td>\n",
       "      <td>-3.492560</td>\n",
       "      <td>-2.695270</td>\n",
       "      <td>-1.849691</td>\n",
       "      <td>-1.374321</td>\n",
       "      <td>...</td>\n",
       "      <td>1.258419</td>\n",
       "      <td>1.907530</td>\n",
       "      <td>2.280888</td>\n",
       "      <td>1.895242</td>\n",
       "      <td>1.437702</td>\n",
       "      <td>1.193433</td>\n",
       "      <td>1.261335</td>\n",
       "      <td>1.150449</td>\n",
       "      <td>0.804932</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>-1.351779</td>\n",
       "      <td>-2.209006</td>\n",
       "      <td>-2.520225</td>\n",
       "      <td>-3.061475</td>\n",
       "      <td>-3.065141</td>\n",
       "      <td>-3.030739</td>\n",
       "      <td>-2.622720</td>\n",
       "      <td>-2.044092</td>\n",
       "      <td>-1.295874</td>\n",
       "      <td>-0.733839</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.512234</td>\n",
       "      <td>-2.076075</td>\n",
       "      <td>-2.586042</td>\n",
       "      <td>-3.322799</td>\n",
       "      <td>-3.627311</td>\n",
       "      <td>-3.437038</td>\n",
       "      <td>-2.260023</td>\n",
       "      <td>-1.577823</td>\n",
       "      <td>-0.684531</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4998 rows Ã— 141 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6    \\\n",
       "0    -0.112522 -2.827204 -3.773897 -4.349751 -4.376041 -3.474986 -2.181408   \n",
       "1    -1.100878 -3.996840 -4.285843 -4.506579 -4.022377 -3.234368 -1.566126   \n",
       "2    -0.567088 -2.593450 -3.874230 -4.584095 -4.187449 -3.151462 -1.742940   \n",
       "3     0.490473 -1.914407 -3.616364 -4.318823 -4.268016 -3.881110 -2.993280   \n",
       "4     0.800232 -0.874252 -2.384761 -3.973292 -4.338224 -3.802422 -2.534510   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4993  0.608558 -0.335651 -0.990948 -1.784153 -2.626145 -2.957065 -2.931897   \n",
       "4994 -2.060402 -2.860116 -3.405074 -3.748719 -3.513561 -3.006545 -2.234850   \n",
       "4995 -1.122969 -2.252925 -2.867628 -3.358605 -3.167849 -2.638360 -1.664162   \n",
       "4996 -0.547705 -1.889545 -2.839779 -3.457912 -3.929149 -3.966026 -3.492560   \n",
       "4997 -1.351779 -2.209006 -2.520225 -3.061475 -3.065141 -3.030739 -2.622720   \n",
       "\n",
       "           7         8         9    ...       131       132       133  \\\n",
       "0    -1.818286 -1.250522 -0.477492  ...  0.792168  0.933541  0.796958   \n",
       "1    -0.992258 -0.754680  0.042321  ...  0.538356  0.656881  0.787490   \n",
       "2    -1.490659 -1.183580 -0.394229  ...  0.886073  0.531452  0.311377   \n",
       "3    -1.671131 -1.333884 -0.965629  ...  0.350816  0.499111  0.600345   \n",
       "4    -1.783423 -1.594450 -0.753199  ...  1.148884  0.958434  1.059025   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "4993 -2.664816 -2.090137 -1.461841  ...  1.757705  2.291923  2.704595   \n",
       "4994 -1.593270 -1.075279 -0.976047  ...  1.388947  2.079675  2.433375   \n",
       "4995 -0.935655 -0.866953 -0.645363  ... -0.472419 -1.310147 -2.029521   \n",
       "4996 -2.695270 -1.849691 -1.374321  ...  1.258419  1.907530  2.280888   \n",
       "4997 -2.044092 -1.295874 -0.733839  ... -1.512234 -2.076075 -2.586042   \n",
       "\n",
       "           134       135       136       137       138       139  140  \n",
       "0     0.578621  0.257740  0.228077  0.123431  0.925286  0.193137    1  \n",
       "1     0.724046  0.555784  0.476333  0.773820  1.119621 -1.436250    1  \n",
       "2    -0.021919 -0.713683 -0.532197  0.321097  0.904227 -0.421797    1  \n",
       "3     0.842069  0.952074  0.990133  1.086798  1.403011 -0.383564    1  \n",
       "4     1.371682  1.277392  0.960304  0.971020  1.614392  1.421456    1  \n",
       "...        ...       ...       ...       ...       ...       ...  ...  \n",
       "4993  2.451519  2.017396  1.704358  1.688542  1.629593  1.342651    0  \n",
       "4994  2.159484  1.819747  1.534767  1.696818  1.483832  1.047612    0  \n",
       "4995 -3.221294 -4.176790 -4.009720 -2.874136 -2.008369 -1.808334    0  \n",
       "4996  1.895242  1.437702  1.193433  1.261335  1.150449  0.804932    0  \n",
       "4997 -3.322799 -3.627311 -3.437038 -2.260023 -1.577823 -0.684531    0  \n",
       "\n",
       "[4998 rows x 141 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"ecg_autoencoder_dataset.csv\" , header = None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ace7cc9",
   "metadata": {
    "id": "7ace7cc9"
   },
   "outputs": [],
   "source": [
    "X = df.drop([140], axis = 1)\n",
    "y = df[140]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc8c6e3b",
   "metadata": {
    "id": "bc8c6e3b"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "592b78e5",
   "metadata": {
    "id": "592b78e5"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train =  scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a22a664",
   "metadata": {
    "id": "6a22a664"
   },
   "outputs": [],
   "source": [
    "encoder = Sequential([Dense(64, activation = 'relu', input_shape = (X_train.shape[1],))])\n",
    "decoder = Sequential([Dense(X_train.shape[1], activation = 'sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4125f230",
   "metadata": {
    "id": "4125f230"
   },
   "outputs": [],
   "source": [
    "autoencoder = Sequential([encoder, decoder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8063605",
   "metadata": {
    "id": "b8063605"
   },
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd106b56",
   "metadata": {
    "id": "cd106b56",
    "outputId": "601d061a-0d66-4211-b025-567294a9b57f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "55/55 [==============================] - 9s 34ms/step - loss: 0.9761 - accuracy: 0.0129 - val_loss: 0.7760 - val_accuracy: 0.0313\n",
      "Epoch 2/100\n",
      "55/55 [==============================] - 1s 18ms/step - loss: 0.7318 - accuracy: 0.0460 - val_loss: 0.7086 - val_accuracy: 0.0647\n",
      "Epoch 3/100\n",
      "55/55 [==============================] - 1s 20ms/step - loss: 0.6937 - accuracy: 0.0649 - val_loss: 0.6883 - val_accuracy: 0.0720\n",
      "Epoch 4/100\n",
      "55/55 [==============================] - 1s 17ms/step - loss: 0.6782 - accuracy: 0.0709 - val_loss: 0.6775 - val_accuracy: 0.0853\n",
      "Epoch 5/100\n",
      "55/55 [==============================] - 1s 19ms/step - loss: 0.6690 - accuracy: 0.0729 - val_loss: 0.6704 - val_accuracy: 0.0913\n",
      "Epoch 6/100\n",
      "55/55 [==============================] - 1s 24ms/step - loss: 0.6626 - accuracy: 0.0829 - val_loss: 0.6648 - val_accuracy: 0.1007\n",
      "Epoch 7/100\n",
      "55/55 [==============================] - 2s 29ms/step - loss: 0.6563 - accuracy: 0.0921 - val_loss: 0.6594 - val_accuracy: 0.1140\n",
      "Epoch 8/100\n",
      "55/55 [==============================] - 1s 21ms/step - loss: 0.6524 - accuracy: 0.1098 - val_loss: 0.6561 - val_accuracy: 0.1287\n",
      "Epoch 9/100\n",
      "55/55 [==============================] - 1s 19ms/step - loss: 0.6494 - accuracy: 0.1209 - val_loss: 0.6534 - val_accuracy: 0.1460\n",
      "Epoch 10/100\n",
      "55/55 [==============================] - 1s 26ms/step - loss: 0.6468 - accuracy: 0.1326 - val_loss: 0.6512 - val_accuracy: 0.1600\n",
      "Epoch 11/100\n",
      "55/55 [==============================] - 2s 28ms/step - loss: 0.6447 - accuracy: 0.1507 - val_loss: 0.6494 - val_accuracy: 0.1713\n",
      "Epoch 12/100\n",
      "55/55 [==============================] - 2s 32ms/step - loss: 0.6430 - accuracy: 0.1558 - val_loss: 0.6480 - val_accuracy: 0.1760\n",
      "Epoch 13/100\n",
      "55/55 [==============================] - 1s 19ms/step - loss: 0.6416 - accuracy: 0.1607 - val_loss: 0.6468 - val_accuracy: 0.1827\n",
      "Epoch 14/100\n",
      "55/55 [==============================] - 1s 21ms/step - loss: 0.6405 - accuracy: 0.1664 - val_loss: 0.6458 - val_accuracy: 0.1907\n",
      "Epoch 15/100\n",
      "55/55 [==============================] - 1s 24ms/step - loss: 0.6395 - accuracy: 0.1727 - val_loss: 0.6451 - val_accuracy: 0.1853\n",
      "Epoch 16/100\n",
      "55/55 [==============================] - 2s 31ms/step - loss: 0.6387 - accuracy: 0.1710 - val_loss: 0.6443 - val_accuracy: 0.1833\n",
      "Epoch 17/100\n",
      "55/55 [==============================] - 2s 30ms/step - loss: 0.6379 - accuracy: 0.1764 - val_loss: 0.6437 - val_accuracy: 0.1867\n",
      "Epoch 18/100\n",
      "55/55 [==============================] - 1s 27ms/step - loss: 0.6373 - accuracy: 0.1784 - val_loss: 0.6432 - val_accuracy: 0.1913\n",
      "Epoch 19/100\n",
      "55/55 [==============================] - 2s 33ms/step - loss: 0.6367 - accuracy: 0.1807 - val_loss: 0.6427 - val_accuracy: 0.2033\n",
      "Epoch 20/100\n",
      "55/55 [==============================] - 2s 32ms/step - loss: 0.6362 - accuracy: 0.1890 - val_loss: 0.6422 - val_accuracy: 0.1953\n",
      "Epoch 21/100\n",
      "55/55 [==============================] - 2s 32ms/step - loss: 0.6356 - accuracy: 0.1824 - val_loss: 0.6416 - val_accuracy: 0.2007\n",
      "Epoch 22/100\n",
      "55/55 [==============================] - 1s 26ms/step - loss: 0.6351 - accuracy: 0.1893 - val_loss: 0.6412 - val_accuracy: 0.1960\n",
      "Epoch 23/100\n",
      "55/55 [==============================] - 1s 27ms/step - loss: 0.6347 - accuracy: 0.1910 - val_loss: 0.6408 - val_accuracy: 0.2040\n",
      "Epoch 24/100\n",
      "55/55 [==============================] - 2s 28ms/step - loss: 0.6342 - accuracy: 0.1975 - val_loss: 0.6406 - val_accuracy: 0.2040\n",
      "Epoch 25/100\n",
      "55/55 [==============================] - 1s 24ms/step - loss: 0.6339 - accuracy: 0.1973 - val_loss: 0.6403 - val_accuracy: 0.2053\n",
      "Epoch 26/100\n",
      "55/55 [==============================] - 1s 27ms/step - loss: 0.6335 - accuracy: 0.2053 - val_loss: 0.6401 - val_accuracy: 0.2040\n",
      "Epoch 27/100\n",
      "55/55 [==============================] - 2s 30ms/step - loss: 0.6332 - accuracy: 0.2067 - val_loss: 0.6397 - val_accuracy: 0.2167\n",
      "Epoch 28/100\n",
      "55/55 [==============================] - 1s 21ms/step - loss: 0.6329 - accuracy: 0.2084 - val_loss: 0.6395 - val_accuracy: 0.2147\n",
      "Epoch 29/100\n",
      "55/55 [==============================] - 1s 24ms/step - loss: 0.6326 - accuracy: 0.2115 - val_loss: 0.6392 - val_accuracy: 0.2167\n",
      "Epoch 30/100\n",
      "55/55 [==============================] - 1s 25ms/step - loss: 0.6323 - accuracy: 0.2121 - val_loss: 0.6390 - val_accuracy: 0.2153\n",
      "Epoch 31/100\n",
      "55/55 [==============================] - 2s 29ms/step - loss: 0.6320 - accuracy: 0.2153 - val_loss: 0.6388 - val_accuracy: 0.2147\n",
      "Epoch 32/100\n",
      "55/55 [==============================] - 1s 21ms/step - loss: 0.6318 - accuracy: 0.2147 - val_loss: 0.6387 - val_accuracy: 0.2100\n",
      "Epoch 33/100\n",
      "55/55 [==============================] - 1s 26ms/step - loss: 0.6315 - accuracy: 0.2250 - val_loss: 0.6384 - val_accuracy: 0.2067\n",
      "Epoch 34/100\n",
      "55/55 [==============================] - 1s 25ms/step - loss: 0.6313 - accuracy: 0.2227 - val_loss: 0.6383 - val_accuracy: 0.2080\n",
      "Epoch 35/100\n",
      "55/55 [==============================] - 2s 33ms/step - loss: 0.6311 - accuracy: 0.2261 - val_loss: 0.6381 - val_accuracy: 0.2107\n",
      "Epoch 36/100\n",
      "55/55 [==============================] - 2s 29ms/step - loss: 0.6309 - accuracy: 0.2264 - val_loss: 0.6380 - val_accuracy: 0.2167\n",
      "Epoch 37/100\n",
      "55/55 [==============================] - 2s 31ms/step - loss: 0.6307 - accuracy: 0.2310 - val_loss: 0.6378 - val_accuracy: 0.2260\n",
      "Epoch 38/100\n",
      "55/55 [==============================] - 1s 27ms/step - loss: 0.6305 - accuracy: 0.2304 - val_loss: 0.6377 - val_accuracy: 0.2200\n",
      "Epoch 39/100\n",
      "55/55 [==============================] - 2s 28ms/step - loss: 0.6303 - accuracy: 0.2301 - val_loss: 0.6375 - val_accuracy: 0.2220\n",
      "Epoch 40/100\n",
      "55/55 [==============================] - 2s 30ms/step - loss: 0.6301 - accuracy: 0.2333 - val_loss: 0.6374 - val_accuracy: 0.2200\n",
      "Epoch 41/100\n",
      "55/55 [==============================] - 2s 29ms/step - loss: 0.6299 - accuracy: 0.2353 - val_loss: 0.6372 - val_accuracy: 0.2273\n",
      "Epoch 42/100\n",
      "55/55 [==============================] - 2s 33ms/step - loss: 0.6297 - accuracy: 0.2304 - val_loss: 0.6371 - val_accuracy: 0.2233\n",
      "Epoch 43/100\n",
      "55/55 [==============================] - 2s 34ms/step - loss: 0.6295 - accuracy: 0.2393 - val_loss: 0.6369 - val_accuracy: 0.2300\n",
      "Epoch 44/100\n",
      "55/55 [==============================] - 1s 25ms/step - loss: 0.6293 - accuracy: 0.2399 - val_loss: 0.6368 - val_accuracy: 0.2373\n",
      "Epoch 45/100\n",
      "55/55 [==============================] - 2s 30ms/step - loss: 0.6292 - accuracy: 0.2439 - val_loss: 0.6367 - val_accuracy: 0.2253\n",
      "Epoch 46/100\n",
      "55/55 [==============================] - 2s 32ms/step - loss: 0.6290 - accuracy: 0.2396 - val_loss: 0.6365 - val_accuracy: 0.2313\n",
      "Epoch 47/100\n",
      "55/55 [==============================] - 2s 33ms/step - loss: 0.6289 - accuracy: 0.2419 - val_loss: 0.6364 - val_accuracy: 0.2333\n",
      "Epoch 48/100\n",
      "55/55 [==============================] - 2s 28ms/step - loss: 0.6287 - accuracy: 0.2407 - val_loss: 0.6363 - val_accuracy: 0.2327\n",
      "Epoch 49/100\n",
      "41/55 [=====================>........] - ETA: 0s - loss: 0.6472 - accuracy: 0.2386"
     ]
    }
   ],
   "source": [
    "r = autoencoder.fit(X_train, X_train, epochs = 100, batch_size = 64, validation_data= (X_test, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d8efaf",
   "metadata": {
    "id": "c7d8efaf",
    "outputId": "bec690c5-d353-42c6-87b7-2f26e0f7b4ed"
   },
   "outputs": [],
   "source": [
    "r.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a182b3",
   "metadata": {
    "id": "91a182b3",
    "outputId": "33b63b76-2ae2-46b1-b95b-5561f1bcbc79"
   },
   "outputs": [],
   "source": [
    "plt.plot(r.history['loss'], label = 'loss', color = 'blue')\n",
    "plt.plot(r.history['val_loss'], label = 'val_loss', color = 'green')\n",
    "plt.legend\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63654b4f",
   "metadata": {
    "id": "63654b4f",
    "outputId": "e2f85cc6-0092-4aa1-ab26-f89fbe54088a"
   },
   "outputs": [],
   "source": [
    "plt.plot(r.history['accuracy'], label = 'acc', color = 'red')\n",
    "plt.plot(r.history['val_accuracy'], label = 'val_acc', color = 'green')\n",
    "plt.legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f24eeaf",
   "metadata": {
    "id": "6f24eeaf",
    "outputId": "edd360c4-9a7b-47f6-cd4e-178d430851ab"
   },
   "outputs": [],
   "source": [
    "loss = autoencoder.evaluate(X_test, X_test)\n",
    "print(f'Test Loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf52a7b",
   "metadata": {
    "id": "7bf52a7b",
    "outputId": "4a4c5d64-8523-4c64-b1fa-742833ae7967"
   },
   "outputs": [],
   "source": [
    "decoded_data = autoencoder.predict(X_test)\n",
    "mse = np.mean(np.power(X_test - decoded_data, 2), axis = 1)\n",
    "threshold = np.percentile(mse, 95)\n",
    "\n",
    "outliers = mse> threshold\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, outliers))\n",
    "print(\"Classification report:\\n\", classification_report(y_test, outliers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c09e34",
   "metadata": {
    "id": "45c09e34"
   },
   "outputs": [],
   "source": [
    "num_outliers = np.sum(outliers)\n",
    "num_anomalies = np.sum(y_test[outliers] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b387e2",
   "metadata": {
    "id": "d7b387e2",
    "outputId": "c1e4759d-1ff8-4351-9d4d-1e689249105c"
   },
   "outputs": [],
   "source": [
    "print(f'Number of outliers: {num_outliers}')\n",
    "print(f'Number of anomalies: {num_anomalies}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918a56f1",
   "metadata": {
    "id": "918a56f1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f6d36f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c651215",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe91df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# encoder = Sequential([Dense(64,activation='relu',input_shape=(X_train.shape[1],))])\n",
    "# decoder = Sequential([Dense(X_train.shape[1],activation='sigmoid')])\n",
    "\n",
    "# autoencoder = Sequential([encoder,decoder])\n",
    "\n",
    "\n",
    "# autoencoder.compile(optimizer='adam',loss='mean_squared_error',metrics=['accuracy'])\n",
    "\n",
    "# r = autoencoder.fit(X_train,X_train,epochs=100,validation_data=[X_test,X_test],batch_size=64)\n",
    "\n",
    "# r.history.keys()\n",
    "\n",
    "# plt.plot()\n",
    "\n",
    "\n",
    "# loss = autoencoder.evaluate(X_test,X_test)\n",
    "# decoded_data = autoencoder.predict(X_test)\n",
    "\n",
    "# mse = np.mean(np.power(X_test-decoded_data,2),axis=1)\n",
    "# threshold = np.percentile(mse,95)\n",
    "\n",
    "# outlier = mse > threshold\n",
    "\n",
    "# print('confusion_matrix =',confusion_matrix(Y_test,outlier)\n",
    "      \n",
    "# print('Classification_report =  ',classification_report(Y_test,outliers))\n",
    "      \n",
    "# total_outliers = np.sum(outliers)\n",
    "# print('total out :',total_outliers)\n",
    "      \n",
    "# total_anomalies = np.sum(Y_test[outliers]==1)\n",
    "      \n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
